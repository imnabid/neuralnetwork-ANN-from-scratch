{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Digit_recognizer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ058lXl22Ut"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyG91oGd64zg"
      },
      "source": [
        "sigmoid forward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6N4afa43nyh"
      },
      "source": [
        "def sigmoid(Z):\r\n",
        "  return (1/(1+np.exp(-Z))), Z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9lPp-Wy7C7p"
      },
      "source": [
        "relu forward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1a0aUfp7F5U"
      },
      "source": [
        "def relu(Z):\r\n",
        "  return np.maximum(0, Z), Z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqXRWXTyGaNU"
      },
      "source": [
        "sigmoid backward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVOCT0JBGdMV"
      },
      "source": [
        "def sigmoid_backward(dA, cache):\r\n",
        "    Z = cache\r\n",
        "    \r\n",
        "    s = 1/(1+np.exp(-Z))\r\n",
        "    dZ = dA * s * (1-s)\r\n",
        "    \r\n",
        "    assert (dZ.shape == Z.shape)\r\n",
        "    \r\n",
        "    return dZ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4NgDMqKG3UN"
      },
      "source": [
        "relu backward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSTd9Y5DG4dk"
      },
      "source": [
        "def relu_backward(dA, Z):\r\n",
        "  dZ = np.array(dA, copy=True) # just converting dz to a correct object.\r\n",
        "  \r\n",
        "  # When z <= 0, you should set dz to 0 as well. \r\n",
        "  dZ[Z <= 0] = 0\r\n",
        "    \r\n",
        "  assert (dZ.shape == Z.shape)\r\n",
        "    \r\n",
        "  return dZ\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S74zLAnm8Y30"
      },
      "source": [
        "**Initializing parameters (Weights and biases)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksuwaxRF8ZMw"
      },
      "source": [
        "def initialize_parameters(layers):\r\n",
        "  L = len(layers)\r\n",
        "  parameters = {}\r\n",
        "  np.random.seed(42)\r\n",
        "  for l in range(1,L):\r\n",
        "    parameters['W' + str(l)] = np.random.randn(layers[l], layers[l-1]) * 0.01\r\n",
        "    parameters['b' + str(l)] = np.zeros((layers[l], 1))\r\n",
        "\r\n",
        "  return parameters\r\n",
        "    \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnKaa9q98Z8i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG2ULWwa3sMj"
      },
      "source": [
        "**Forward Propagation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1eP2q6i4aqD"
      },
      "source": [
        "**1. linear_forward**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE0DHmwY3v-F"
      },
      "source": [
        "def linear_forward(A_prev, W, b):\r\n",
        "  Z = np.dot(W,A_prev) + b\r\n",
        "  cache = A_prev, W, b #for backward propagation\r\n",
        "\r\n",
        "  assert (Z.shape == (W.shape[0], A_prev.shape[1]))\r\n",
        "\r\n",
        "  return Z, cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNci5Rmf4iZs"
      },
      "source": [
        "**2. linear_activation_forward**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqDdr5jQ4nc-"
      },
      "source": [
        "def linear_forward_activation(A_prev, W, b, activation):\r\n",
        "\r\n",
        "  if activation == 'relu':\r\n",
        "    Z, linear_cache = linear_forward(A_prev, W, b)\r\n",
        "    A, act_cache = relu(Z)\r\n",
        "  elif activation == 'sigmoid':\r\n",
        "    Z, linear_cache = linear_forward(A_prev, W, b)\r\n",
        "    A, act_cache = sigmoid(Z)   \r\n",
        "  \r\n",
        "  cache = (linear_cache, act_cache) \r\n",
        "  assert ( A.shape == (Z.shape)) \r\n",
        "\r\n",
        "  return A, cache\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUAAe-SG7kIc"
      },
      "source": [
        " **3. L_layer_forward**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeVziZyZA4vv"
      },
      "source": [
        "def L_forward(X, parameters):\r\n",
        "  A = X\r\n",
        "  L = len(parameters)//2\r\n",
        "  caches =[]\r\n",
        "  #activations till the second last layer\r\n",
        "  for l in range(1,L):\r\n",
        "    A_prev = A\r\n",
        "    A, cache =  linear_forward_activation(A_prev,\r\n",
        "                                          parameters['W' + str(l)],\r\n",
        "                                          parameters['b' + str(l)],\r\n",
        "                                          'relu')\r\n",
        "    caches.append(cache)\r\n",
        "\r\n",
        "  #activation for the last layer\r\n",
        "  AL, cache = linear_forward_activation(A,\r\n",
        "                                        parameters['W' + str(L)],\r\n",
        "                                        parameters['b' + str(L)],\r\n",
        "                                        'sigmoid')  \r\n",
        "  caches.append(cache)\r\n",
        "  assert (AL.shape == (1, X.shape[1]))\r\n",
        "\r\n",
        "  return AL, caches\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA9NHkEtPL8d"
      },
      "source": [
        "**Compute cost**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5LcFBCdPPgA"
      },
      "source": [
        "def cost(AL, Y):\r\n",
        "  m = Y.shape[1]\r\n",
        "  cost = (-1/m)*np.sum(np.multiply(Y,np.log(AL))+np.multiply(1-Y,np.log(1-AL)))\r\n",
        "  cost = np.squeeze(cost)\r\n",
        "  assert cost.shape == ()\r\n",
        "\r\n",
        "  return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnGX_lInBbgy"
      },
      "source": [
        "**Backward Propagaton**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fzpnqpPBoke"
      },
      "source": [
        "**1.linear_backward**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmM9Yz2xBvOu"
      },
      "source": [
        "def linear_backward(dZ, cache):\r\n",
        "\r\n",
        "  A_prev, W, b = cache\r\n",
        "  m = A_prev.shape[1]\r\n",
        "\r\n",
        "  dW = 1/m*np.dot(dZ,A_prev.T)\r\n",
        "  db = 1/m*np.sum(dZ,axis=1,keepdims=True)\r\n",
        "  dA_prev = np.dot(W.T,dZ)\r\n",
        "\r\n",
        "  assert (dW.shape == W.shape)\r\n",
        "  assert (db.shape == b.shape)\r\n",
        "  assert (dA_prev.shape == A_prev.shape)\r\n",
        "\r\n",
        "  return dA_prev, dW, db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ7U260yEXJq"
      },
      "source": [
        "**2.linear_activation_backward**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHR2MCB5EVJN"
      },
      "source": [
        "def linear_activation_backward(dA, cache,  activation):\r\n",
        "  linear_cache, act_cache = cache \r\n",
        "\r\n",
        "  if activation == 'relu':\r\n",
        "    dZ = relu_backward(dA, act_cache)\r\n",
        "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\r\n",
        "\r\n",
        "  elif activation == 'sigmoid':\r\n",
        "    dZ = sigmoid_backward(dA, act_cache)\r\n",
        "    dA_prev, dW, db = linear_backward(dZ, linear_cache)  \r\n",
        "\r\n",
        "  return dA_prev, dW, db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcYZuG60OrlP"
      },
      "source": [
        "**3.L_backward**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bj_D5ZkFOuRm"
      },
      "source": [
        "def L_backward(AL, Y, caches):\r\n",
        "\r\n",
        "  dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\r\n",
        "  gradients = {}\r\n",
        "  L = len(caches)\r\n",
        "  \r\n",
        "  #gradient of Lth layer:\r\n",
        "  dA, dW, db = linear_activation_backward(dAL, caches[L-1],  activation ='sigmoid')\r\n",
        "  gradients['dA' + str(L-1)] = dA\r\n",
        "  gradients['dW' + str(L)] = dW\r\n",
        "  gradients['db' + str(L)] = db\r\n",
        "\r\n",
        "  #For other layers\r\n",
        "  for l in reversed(range(L-1)):\r\n",
        "    \r\n",
        "    dA_prev, dW, db = linear_activation_backward(gradients[\"dA\" + str(l + 1)], caches[l],  activation ='relu')\r\n",
        "    gradients['dA' + str(l)] = dA_prev\r\n",
        "    gradients['dW' + str(l+1)] = dW\r\n",
        "    gradients['db' + str(l+1)] = db\r\n",
        "   \r\n",
        "\r\n",
        "  return gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ParnIadW83N"
      },
      "source": [
        "**Updating parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPUZ0bD1W_GX"
      },
      "source": [
        "def update_params(parameters, gradients, learning_rate):\r\n",
        "  L = len(parameters)//2\r\n",
        "    for l in range(1,L+1):\r\n",
        "     parameters['W' + str(l)] = parameters['W' + str(l)] - learning_rate * gradients['dW' + str(l)] \r\n",
        "     parameters['b' + str(l)] = parameters['b' + str(l)] - learning_rate * gradients['db' + str(l)] \r\n",
        "\r\n",
        "  return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kk2bSlaEX-wV"
      },
      "source": [
        "**L_layer nn model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wyCwsCpZMKL"
      },
      "source": [
        "def L_layer_model(X, Y, layers, learning_rate, iterations, print_cost = False):\r\n",
        "  np.random.seed(42)\r\n",
        "  costs = []\r\n",
        "  parameters = initialize_parameters(layers)\r\n",
        "\r\n",
        "  for i in range(iterations):\r\n",
        "    AL, caches = L_forward(X, parameters)\r\n",
        "\r\n",
        "    c = cost(AL, Y)\r\n",
        "    gradients = L_backward(AL, Y, caches)\r\n",
        "\r\n",
        "    parameters = update_params(parameters, gradients, learning_rate)\r\n",
        "\r\n",
        "    if print_cost and i % 100 == 0:\r\n",
        "            print (\"Cost after iteration %i: %f\" % (i, c))\r\n",
        "            costs.append(c)\r\n",
        "\r\n",
        "  return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C_vLIK_c5Z9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEWf3WM3Jdml"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}